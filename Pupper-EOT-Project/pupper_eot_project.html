<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Team 1 - PupMappers</title>

    <link rel="stylesheet" type="text/css" href="../page_style.css">
    <script src="../scripts.js"></script>
</head>
<body>
    <header>
        <div class="a">
            <h1>End of Term Project: SLAM</h1>
        </div>
    </header>

    <a class="links" href="../index.html">Main Website</a>
    <a class="links" href="../Misty-Blockly/misty_blockly.html">Misty Blockly</a>
    <a class="other_links" href="../Misty-Python/misty_python.html">Misty Python</a>
    <a class="links" href="../Pupper-Assembly/pupper_assembly.html">Pupper Assembly</a>
    <a class="other_links" href="../Pupper-Racing/pupper_racing.html">Pupper Racing</a>
    <a class="other_links" href="pupper_eot_project.html">End of Term Project: SLAM</a>

    <br><br>

    <h2>PupMappers</h2>
    <div class="section">
        <p>
            Our final project was focused on using MandDang's Mini Pupper 1 to implement SLAM (Simultaneous Localization and Mapping). The goal was to enable the robot to
            autonomously navigate and map its environment, enhancing its capability to perform complex tasks. Below, you'll find detailed information about our proposal, demo videos,
            updates on our progress, and our final results.
        </p>

        <div class="section">
            <button onclick="toggle_visibility('Proposal', this)" class="expandable plus">Proposal (May 7, 2024)</button>
            <div class="hidden btn_group" id="Proposal">
                <p>
                    Below you will find the proposal that was made. Mapping and localization is an important problem to solve for many robotic systems.
                    Giving the Mini Pupper the ability to localize itself and map its surroundings would further its capabilities of performing more complicated tasks autonomously.
                    Additionally it has many complex components what would and did offer great learning opportunities.
                </p>

                <div class="section">
                    <button onclick="toggle_visibility('proposal_presentation', this)" class="expandable plus">Presentation</button>
                    <div class="hidden btn_group" id="proposal_presentation">
                        <p>
                            We gave a presentation intended to pitch our idea to our professor and to further investigate the tools we intend to use for the project. We decided
                            on the Mini Pupper robot for our SLAM system. We used ROS as a starting point. MangDang's ROS2 setup has built in SLAM and twist commands for more accurate robot movement.
                            For hardware, we used the LD06 LiDAR unit and the existing servos on the Pupper. Rviz was used to visualize the LiDAR data.
                        </p>
                        
                        <object data="Project Pitch.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('proposal_videos', this)" class="expandable plus">Videos and Background</button>
                    <div class="hidden btn_group" id="proposal_videos">
                        <p>
                            Below are some videos related to our proposal. MangDang's setup and commands can be found in the readme of their
                            <a href="https://github.com/mangdangroboticsclub/mini_pupper_ros">ros repository</a>.
                        </p>

                        <p>
                            Starting into the slam project with MangDang's ros setup was built upon the preface that their setup was already handling the movement. The video below demonstrated
                            that the keyboard simulation moves the pupper atleast in simulation. In this simulation, we show how the keyboard can be used to control the Mini Pupper's movements,
                            which is essential for our mapping tasks. We utilize RViz as part of the simulation to visualize the LiDAR data and ensure accurate mapping of the room.
                            This setup allows us to manually guide the Mini Pupper through the environment, collecting data for the SLAM system to create a detailed map.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/X7Mf_iLr0XE" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            The second preface to using MangDang's ros setup was that they had a working version of slam (in simulation). This video showcases the capacity to test the pupper in simulaiton.
                            Using ROS 2 and Gazebo, which is a robot simulation environment, we were able to interact with and control the Mini Pupper. Gazebo can be used independently or integrated
                            with ROS 2 using a set of packages. In this simulation, we utilized keyboard commands embedded within ROS 2 to control the robot and map its surroundings.
                            The visualized map is displayed using RViz, demonstrating our progress in achieving accurate mapping through simulation.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/VzPBjBh2btA" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            The last video was more of a fun slide element. The video shows the Mini Pupper running in circles, which looks very similar to a loading screen.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/1fWf68K62EI" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_0', this)" class="expandable plus">Hello World Demo (May 16, 2024)</button>
            <div class="hidden btn_group" id="Demo_0">
                <p>
                    Our initial demo involved setting up the Mini Pupper and running a simple "Hello World" program to ensure all components were functioning correctly. It would be accurate
                    to describe this as the first real life interaction the pupper had toward its goal.
                </p>

                <div class="section">
                    <button onclick="toggle_visibility('presentation_0', this)" class="expandable plus">Update Presentation</button>
                    <div class="hidden btn_group" id="presentation_0">
                        <p>
                            We presented our initial findings and progress, focusing on the setup process and initial tests of the Mini Pupper.
                        </p>
                        <object data="Project Update.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('videos_0', this)" class="expandable plus">Update Videos and Background</button>
                    <div class="hidden btn_group" id="videos_0">
                        <p>
                            Below are some update videos showcasing our progress. Most of these require an edit to the teleop_twist_keyboard. The file can be found at
                            /opt/ros/humble/lib/python3.10/site-packages/teleop_twist_keyboard.py. Edits to this file will be reflected when using the
                            `ros2 run teleop_twist_keyboard teleop_twist_keyboard` command.
                        </p>

                        <p>
                            This video demonstrates the keyboard commands on the actual Mini Pupper. In this demo, we show how the keyboard can be used to control the Mini Pupper's
                            movements, which is essential for our mapping tasks. This setup allows us to manually guide the Mini Pupper through the environment, collecting data for the SLAM
                            system to create a detailed map.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/d-B7l_8jRX8" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            Although the goal for projects by this point is to use the real pupper, testing steps are always important to consistent progress to larger goals.
                            Considering my background in software I wanted to do something more complex that would move the team torward the goal (via movement) and demo how
                            movement can work for my teammates who also had personal assignments. When I thought of interesting dead reckoning movement schemes my mind went to
                            the classic DVD logo. It bounces off of walls that are arbitrarily large. As long as drift isn't too bad (spoiler, it was) the dvd logo could bounce around
                            for awhile. Admitedly I was ambitious in setting my stretch goal of integrating the lidar we had not yet received.<br>

                            <br>

                            The video shown above shows the classic dvd logo (created in Python) which simulated how many steps can be taken before a wall is hit and then the pupper simulated
                            to show what the pupper would look like bouncing off of said walls. A simulation approach was taken in order to minimize potential problems on the pupper before they
                            were necessary. This is a classic example of turning one problem into another. Instead of the pupper handling its environment as it enteracts with it, a contained
                            simulation of a completely different object is used with its own rules. The results of those rules are reflected by the pupper when it takes action.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/PcCbTTpzA0U" allowfullscreen></iframe>
                        <p>
                            It should be noted that the video is running at 2x speed for compression and reader retension.
                        </p>

                        <br>

                        <p>
                            The concept for the pupper demo above was mostly described in the previous excerpt. Due to work space there is audio from my roommates. I chose to leave it in for entertainment
                            as well as for the pupper movement noises. Although the simulation is not shown, the actions were mostly correct. The bounding box for the dvd logo was made much smaller, which helped with
                            the deviation. There are a couple points where it seems to avoid collision with real life objects, but those are actually just pure coincidence. The biggest struggles for this project
                            were finding the teleop_twist_keyboard.py file, /opt/ros/humble/lib/python3.10/site-packages/teleop_twist_keyboard.py, which I had to rip grep for and time (3 days).
                        </p>
                        <iframe src="https://youtube.com/embed/RzNltnV_vV0" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            Using the ROS 2 repository with the Mini Pupper, the team used a provided program. The team's goal with the project was to generate a map using SLAM on the Mini Pupper.
                            Though tangential moving to the beat of music is a fun exploration of the software available using ros 2. It also explored package editing (colcon build) which would be an integral part
                            of the project.introducing more movements for each servo of the robot.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/ZGS4t3Fdrqw" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            Using the ROS2 repository with the Mini Pupper had the team use a provided program to manually control the robot using a computer keyboard.
                            The team's goal with the project was to generate a map using SLAM on the pupper. This would potentially require the pupper to turn a certain amount.
                            The artifact had modified that provided code to turn the Mini Pupper 90 degrees in either the clockwise or counter directions.
                            This will better enable the team to move the robot and generate SLAM maps. Leading the team forward to attempt developing autonomous movement to generate
                            additional area maps.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/jxYhrxGIf5A" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            My goal for this project was to figure out how to efficiently move and turn the Mini Pupper after the recent update to run it using ROS. Based on a movement program
                            for twist commands in the existing Mini Pupper ROS repository, I created my own that had the robot move forward, turn 180 degrees, and then return to its starting position.
                            The robot was fairly stable and consistent going forward, but I did have some difficulty turning the robot exactly 180 degrees as is apparent in the video.
                            I found there was a decent amount of variability in how much the Mini Pupper would turn each time while running the same program. I hope to be able to develop a more
                            robust system for turning later on.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/3EvsSjVtG7w" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This video demonstrates the LiDAR module in ROS 2. The Mini Pupper is mounted with a MANGDANG STL-06P LiDAR module, which performs well as intended.
                            The LiDAR system uses laser pulses to measure distances to surrounding objects, creating a detailed 2D map of the environment. The video shows lines of
                            different colors signifying various objects detected by the LiDAR. The darker the color, the more likely it is recognized as a solid object. This color coding
                            helps in distinguishing between different types of objects and understanding their solidity. The LiDAR data is visualized in RViz, a 2D visualization tool for ROS,
                            which allows us to see the environment from the robot's perspective and ensures accurate mapping and obstacle detection.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/Bpg9jhSw23E" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_1', this)" class="expandable plus">Update Demo 1 (May 23, 2024)</button>
            <div class="hidden btn_group" id="Demo_1">
                <p>
                    In this demo, we focused on improving the accuracy of the robot's movements and integrating more sensors.
                </p>
                <img src="demo_1.png" alt="Map of a room created during Demo 1" class="small_img">
                <p>
                    For the first demo, we attempted to solidify the LiDAR integration with the Mini Pupper to generate a map outside of a simulation. We explored two approaches for integrating the LiDAR: initially reviewing and utilizing the existing Mini Pupper SLAM documentation to begin generating maps, and subsequently attempting to modify or develop our own SLAM implementation to achieve better control over the mapping process. The resulting map demonstrates the Mini Pupper's ability to accurately navigate and map its environment. By integrating additional sensors and refining the robot's movements, we were able to create a detailed and accurate representation of the room.<br>
                </p>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_2', this)" class="expandable plus">Update Demo 2 (May 30, 2024)</button>
            <div class="hidden btn_group" id="Demo_2">
                <p>
                    Our final in class demo showcases an implementation of the SLAM system on the Mini Pupper, including semi-naive navigation. This technically meats the requirement of demo 2,
                    but our team was not quite satisfied with the overall results (which is why we took the opportunity we were given to continue working on the pupper into finals week).<br>
                </p>
                
                <div class="section">
                    <button onclick="toggle_visibility('final_presentation', this)" class="expandable plus">Presentation</button>
                    <div class="hidden btn_group" id="final_presentation">
                        <p>
                            The presentation shown below is the last update on the project done before the class. As such there is reflective content with a bent toward finality. Any further work
                            was optional. Especially since even with a 0 on our final turn in, would still result in all of us getting an A in the course.
                        </p>
                        <object data="Final Project Update.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('videos_1', this)" class="expandable plus">Update Videos and Background</button>
                    <div class="hidden btn_group" id="videos_1">
                    
                        <iframe src="https://youtube.com/embed/kvn6AokmaWs" allowfullscreen></iframe>

                        <p>
                            There are a few things to note about the video above.
                        </p>
                        <ul>
                            <li>
                                The "leash" as some people have called it was due to the pupper's battery dieing right as we started recording for the demo.
                                The pupper has a power cord and so we got around the issue by connecting an extension cord. John was holding the power cord to keep it from interfearing with the pupper's motion.
                            </li>
                            <li>
                                The pupper is much closer to the ground than in previous videos. After discussing the pupper's proclivity to tipping over during turns with the developer we learned that
                                the center of mass can just be lowered. This also keeps the pupper from kicking the ground as much.
                            </li>
                            <li>
                                The teleop_twist_keyboard command is different than what is given out of the box. I made some external libraries to "think" through the various functions that we still needed for
                                localization and mapping. One of those changes draws graphs (in png form) that allows me to see what the pupper was thinking about its environment. This drawing step causes the pupper
                                to appear to be thinking for longer. Though part of the extra time is due to the pupper just not being a fast thinker.
                            </li>
                            <li>
                                An occupancy grid was created by ROS using lidar before the start of the video. ROS's map.yaml which should store the pupper's location on the map is not correct. This causes there
                                to be a "where am i" in the image. Though we did solve the question, the accuracy is not perfect and therefore the pupper in the video actually thinks there is a table infront of it,
                                which is why it does so much turning.
                            </li>
                        </ul>

                        <p>
                            Regardless of the setbacks, the 4 days that led up to this video were insane. A ton of effort and brainstorming was put in to find clever out of the box ways of circumventing our lack of
                            ros2 knowledge, challenges the pupper put up, and how to make something presentable so we could finish the assignment. Luckily after spending most of the term working on ros 2 a method clicked
                            and all that was left was to just do it. We found the problems and tackled them one by one.
                        </p>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('solution', this)" class="expandable plus">Brainstorming a Solution</button>
                    <div class="hidden btn_group" id="solution">
                        <p>
                            After many hours of work in the ros environment, optimizing some of the ros launch files, editing the rviz settings to more closely match our aesthetic needs
                            I realized that I was not going to be able to finish the project entirely in ROS. I just have too little knowledge about making my own ros packages to utilize
                            ros to any inteligable extent. A thought popped into my head, what if we did not have to finish the project in ros.<br>

                            <br>

                            Callen Sun (a developer at MangDang) had published a new readme as well as some cleanup for some of the launch files the morning of this thought process. In that
                            updated readme was a method for converting an occupancy to a .pgm (a file). Pgm files can be read trivially by imageio. John worked with them in ROB 456 for
                            simulated pathfinding. The following methods would be needed to bridge the gap between what ros2 and our full project description.
                        </p>
                        <ul>
                            <li>take_snapshot, create an occupancy grid image of the environment using ros.</li>
                            <li>where_am_i, where is the pupper in the room base on the newest snapshot.</li>
                            <li>where_is_good, where should the pupper go next to ensure full exploration.</li>
                            <li>get_path, find a pixel path from the robots location to a goal location.</li>
                            <li>move, turn a path into a series of commands. As movement is toggled rather than held, commands should be direction and then duration.</li>
                            <li>get_completion_rate, based on an overall map, what percent has been explored.</li>
                        </ul>
                        <p>
                            There were some caviots that needed to be addressed early. If any of the following were not to work the idea would need to be rethought.
                        </p>
                        <ol>
                            <li>
                                A python program must be able to reasonably launch the ros slam program (which runs indefinitely) and then stop it after it has created a satisfactory
                                occupancy grid.
                            </li>
                            <li>The program must be able to tell where the pupper is from the lidar.</li>
                            <li>
                                Movement of the pupper must be consistant enough that drift won't cause major descrepencies between the map and the pupper. We're not using
                                dead reckoning, but we don't want localization to be impossible.
                            </li>
                        </ol>
                        <p>
                            John tackled the first problem immidiately. It turns out that subprocessing.Popen opens its own terminals. Since each terminal is unique, commands like jobs and kill %1
                            will not work. Luckily launching popen is a threaded process and therfore I can use things like wait and time.sleep to ensure the criteria I am looking for in the external
                            program are met.<br>

                            <br>

                            As exhilarating as problem one was to solve, problem two was in a class of its own. Problem two was as beautiful in its simplicity as it was in its aesthetic.
                        </p>
                        <img src="Lidar_Image.jpg" class="medium_img">
                        <p>
                            There is a map.yaml file, but without an imu, the cordinates for the pupper are incorrect (even when standing still). Fortunately a common artifact of lidars are the
                            diagonals that appear to point toward the pupper (as that's where the lasers originated).
                        </p>
                        <img src="Lidar_Grouped_Dots.jpg" class="medium_img">
                        <p>
                            If the region between free space and unknown is converted into groups the groups could be colored unique colors to make them human readible.
                        </p>
                        <img src="Lidar_Grouped_Dot_Lines.jpg" class="medium_img">
                        <p>
                            Larger groups could then be converted into lines.
                        </p>
                        <img src="Lidar_Grouped_Dot_Line_Intersections.jpg" class="medium_img">
                        <p>
                            Given the beginning and the end, m and b can be calculated for y=mx+b. If the map is stepped through at small enough x steps every intersection can be found. The average
                            of the intersections should be the robot position (marked by the red cross shown in all 4 images). Unfortunately this is not perfect. Boring torain can cause fewer lines.
                            Worse, a small area may result in no lines, which would result in no pupper location.<br>
                        </p>

                        <br>

                        <p>
                            Finally, the third problem was actually not an issue. A couple weeks ago Afreez (developer at Mangdang) had suggested lowering the pupper to lower the center of mass.
                            This resulted in less ground kicking and a lower chance of tipping over.<br>
                        </p>

                        <br>

                        <p>
                            The observant reader will notice that by solving these problems take_snapshot and part of where_am_i were solved above. Finding where the pupper is in a lidar reading is only
                            part of the answer. The next step would be to use the landmarks and expected location to match up with the rest of the room's map. Fortunately, for demo 2 (pathfind to a
                            selected location) we only needed a single lidar scan. where_is_good and get_path were able to be easily modified for our lidar maps. Now that all of these pieces were together,
                            we combined them in the teleop_twist_keyboard which controls the movement of the pupper via keyboard. John added a space button and a g button so that the keyboard kept
                            part of its original functionality. Tapping space results in a snapshot and g says to go toward a targeted location. Some measured runs were used to calculate unit conversion.
                        </p>
                        <ul>
                            <li>3 feet/20 pixels</li>
                            <li>3 feet/10.96 seconds</li>
                            <li>720 degrees/29.33 seconds</li>
                            <li>360 degrees/14.88 seconds</li>
                        </ul>
                        <p>The culmination of this work resulted in the video below.</p>
                        <iframe src="https://youtube.com/embed/kvn6AokmaWs" allowfullscreen></iframe>
                        <p>Note this is the same video shown in the previous tab.</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Final_Submission', this)" class="expandable plus">Final Submission (June 12, 2024)</button>
            <div class="hidden btn_group" id="Final_Submission">
                <p>
                    After presenting the last update we were given the option to either keep working on whatever was left or to turn in our hardware
                    and leave progress toward the goal as is. As our group was so close to a proof of concept, we decided to hang onto the hardware for a little bit longer.<br>
                </p>

                <p>
                    Noting that our lidar was working we decided to try to see if we could take lidar scans as we move through a "room" and then combine them. The smallest most reasonable
                    case for this was a hallway.
                </p>

                <img src="Actual Hallway.jpg" class="medium_img">

                <p>Boards were placed at the entrance and exit then all doors were closed. This simulates the case that the pupper completely explores an area.<br></p>
                <p>Once the area was ready, 2 terminals were setup.</p>
                <ul>
                    <li>ros2 launch mini_pupper_bringup bringup.launch.py</li>
                    <li>ros2 run teleop_twist_keyboard teleop_twist_keyboard</li>
                </ul>
                <p>The following steps were taken to clean up the setup.</p>
                <ul>
                    <li>Added . ~/ros2_ws/install/setup.bash to the .profile file. This makes it so we never have to source in order to run the ros commands.</li>
                    <li>bringup.launch.py was unsubscribed from the faux imu and simulated lidar and subscribed to the ld06 lidar topic.</li>
                    <li>slam.rviz was modified to remove the faux imu and tf and to add colorfull lidar scans.</li>
                </ul>
                <p>
                    Below the pupper is moving from the bottom of the hallway to the top of the hallway.
                </p>

                <div>
                    <img src="map1.png" class="side_by_side">
                    <img src="map2.png" class="side_by_side">
                    <img src="map3.png" class="side_by_side">
                </div>

                <div>
                    <img src="map4.png" class="side_by_side">
                    <img src="map5.png" class="side_by_side">
                    <img src="map6.png" class="side_by_side">
                </div>

                <p>Once the pupper finished its rounds, the lidar maps were combined into the final image.</p>
                <img src="overall_maps.png" class="medium_img">
                <p>
                    With the simplist case of the wall the size of the lidar scan does not vary much. This means that walls from all images can just be plotted
                    and a fairly reasonable image can be generated. However if rotation occured, the room was bigger than the scan field, or a wall was ever hidden, this method would not work.
                    Due to time and lack of necessity we decided to stop here.<br>

                    <br>

                    It should be noted that the above combined image was done in post processing rather than algorithmically on the fly.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
