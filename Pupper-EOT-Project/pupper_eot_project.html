<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Team 1 - PupMappers</title>

    <link rel="stylesheet" type="text/css" href="../page_style.css">
    <script src="../scripts.js"></script>
</head>
<body>
    <header>
        <div class="a">
            <h1>End of Term Project: SLAM</h1>
        </div>
    </header>

    <a href="../index.html">Main Website</a>

    <br><br>

    <h2>PupMappers</h2>
    <div class="section">
        <p>
            Our final project is focused on using the Mini Pupper robot to implement SLAM (Simultaneous Localization and Mapping). The goal is to enable the robot to autonomously navigate and map its environment, enhancing its capability to perform complex tasks. Below, you'll find detailed information about our proposal, demo videos, and updates on our progress.
        </p>

        <div class="section">
            <button onclick="toggle_visibility('Proposal', this)" class="expandable plus"> Proposal</button>
            <div class="hidden btn_group" id="Proposal">
                <p>
                    Our final project proposal is to use the Mini Pupper robot to map a room using SLAM. Mapping and localization is an important problem to solve for many robotic systems. Giving the Mini Pupper the ability to localize itself and map its surroundings would further its capabilities of performing more complicated tasks autonomously.
                </p>

                <div class="section">
                    <button onclick="toggle_visibility('proposal_presentation', this)" class="expandable plus"> Presentation</button>
                    <div class="hidden btn_group" id="proposal_presentation">
                        <p>
                            We gave a presentation intended to pitch our idea to our professor and to further investigate the tools we intend to use for the project. We've decided on the Mini Pupper robot for our SLAM system. We are using ROS to give us more SLAM capabilities and access to twist commands for more accurate robot movement. For hardware, we are using a innomaker LiDAR unit and the existing servos on the Pupper. Rviz is used to visualize the LiDAR data.
                        </p>
                        
                        <object data="Project Pitch.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('proposal_videos', this)" class="expandable plus"> Videos</button>
                    <div class="hidden btn_group" id="proposal_videos">
                        <p>
                            Here are some videos related to our proposal:
                        </p>

                        <p>
                            This video demonstrates the keyboard simulation for the Mini Pupper. In this simulation, we show how the keyboard can be used to control the Mini Pupper's movements, which is essential for our mapping tasks. We utilize RViz as part of the simulation to visualize the LiDAR data and ensure accurate mapping of the room. This setup allows us to manually guide the Mini Pupper through the environment, collecting data for the SLAM system to create a detailed map.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/X7Mf_iLr0XE" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This video showcases our ability to map a simulation environment for the Mini Pupper. Using ROS 2 and Gazebo, which is a robot simulation environment, we were able to interact with and control the Mini Pupper. Gazebo can be used independently or integrated with ROS 2 using a set of packages. In this simulation, we utilized keyboard commands embedded within ROS 2 to control the robot and map its surroundings. The visualized map is displayed using RViz, demonstrating our progress in achieving accurate mapping through simulation.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/VzPBjBh2btA" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This video was created for fun to demonstrate the Mini Pupper running in circles. Using RViz, we recorded the simulation of the Mini Pupper executing circular movements. Once completed, we put the robot into a continuous loop, showcasing its ability to perform repetitive tasks accurately within the simulated environment. For fun, we used it as a loading screen.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/1fWf68K62EI" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_0', this)" class="expandable plus"> Hello World Demo</button>
            <div class="hidden btn_group" id="Demo_0">
                <p>
                    Our initial demo involved setting up the Mini Pupper and running a simple "Hello World" program to ensure all components were functioning correctly.
                </p>

                <div class="section">
                    <button onclick="toggle_visibility('presentation_0', this)" class="expandable plus"> Update Presentation</button>
                    <div class="hidden btn_group" id="presentation_0">
                        <p>
                            We presented our initial findings and progress, focusing on the setup process and initial tests of the Mini Pupper.
                        </p>
                        <object data="Project Update.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('videos_0', this)" class="expandable plus"> Update Videos</button>
                    <div class="hidden btn_group" id="videos_0">
                        <p>
                            Here are some update videos showcasing our progress:
                        </p>

                        <p>
                            This video demonstrates the keyboard simulation on the Mini Pupper. In this simulation, we show how the keyboard can be used to control the Mini Pupper's movements, which is essential for our mapping tasks. This setup allows us to manually guide the Mini Pupper through the environment, collecting data for the SLAM system to create a detailed map.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/d-B7l_8jRX8" allowfullscreen></iframe>
                        

                        <br>

                        <p>
                            Although the goal for projects by this point is to use the real pupper, testing steps are always important to consistent progress to larger goals.
                            Considering my background in software I wanted to do something more complex that would move the team torward the goal (via movement) and demo how
                            movement can work for my teammates who also had personal assignments. When I thought of interesting dead reckoning movement schemes my mind went to
                            the classic DVD logo. It bounces off of walls that are arbitrarily large. As long as drift isn't too bad (spoiler, it was) the dvd logo could bounce around
                            for awhile. Admitedly I was ambitious in setting my stretch goal of integrating the lidar we had not yet received.<br>

                            <br>

                            The video shown above shows the classic dvd logo (created in Python) which simulated how many steps can be taken before a wall is hit and then the pupper simulated
                            to show what the pupper would look like bouncing off of said walls. A simulation approach was taken in order to minimize potential problems on the pupper before they
                            were necessary. This is a classic example of turning one problem into another. Instead of the pupper handling its environment as it enteracts with it, a contained
                            simulation of a completely different object is used with its own rules. The results of those rules are reflected by the pupper when it takes action.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/PcCbTTpzA0U" allowfullscreen></iframe>
                        <p>
                            It should be noted that the video is running at 2x speed for compression and reader retension.<br>
                        </p>

                        <br>

                        <p>
                            The concept for the pupper demo above was mostly described in the previous excerpt. Due to work space there is audio from my roommates. I chose to leave it in for entertainment
                            as well as for the pupper movement noises. Although the simulation is not shown, the actions were mostly correct. The bounding box for the dvd logo was made much smaller, which helped with
                            the deviation. There are a couple points where it seems to avoid collision with real life objects, but those are actually just pure coincidence. The biggest struggles for this project
                            were finding the teleop_twist_keyboard.py file, /opt/ros/humble/lib/python3.10/site-packages/teleop_twist_keyboard.py, which I had to rip grep for and time (3 days).
                        </p>
                        <iframe src="https://youtube.com/embed/RzNltnV_vV0" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This artifact was used to further the team's end-of-term project. Using the ROS 2 repository with the Mini Pupper, the team used a provided program. The team's goal with the project was to generate a map using SLAM on the Mini Pupper. This potentially enhanced the movement of the Mini Pupper, introducing more movements for each servo of the robot. The Mini Pupper performed a series of movements and created a dance routine along with a song. This artifact uses Colcon build to rebuild new packages of ROS 2.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/ZGS4t3Fdrqw" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This artifact was used to further the team's end of term project. Using ROS2 repository with the Mini Pupper had the team use a provided program to manually control the robot using a computer keyboard. The team's goal with the project was to generate a map using SLAM on the pupper. This would potentially require the pupper to turn a certain amount. The artifact had modified that provided code to turn the Mini Pupper 90 degrees in either the clockwise or counter directions. This will better enable the team to move the robot and generate SLAM maps. Leading the team forward to attempt developing autonomous movement to generate additional area maps.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/jxYhrxGIf5A" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            My goal for this project was to figure out how to efficiently move and turn the Mini Pupper after the recent update to run it using ROS. Based on a movement program for twist commands in the existing Mini Pupper ROS repository, I created my own that had the robot move forward, turn 180 degrees, and then return to its starting position. The robot was fairly stable and consistent going forward, but I did have some difficulty turning the robot exactly 180 degrees as is apparent in the video. I found there was a decent amount of variability in how much the Mini Pupper would turn each time while running the same program. I hope to be able to develop a more robust system for turning later on.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/3EvsSjVtG7w" allowfullscreen></iframe>
                        
                        <br>

                        <p>
                            This video demonstrates the LiDAR module in ROS 2. The Mini Pupper is mounted with a MANGDANG STL-06P LiDAR module, which performs well as intended. The LiDAR system uses laser pulses to measure distances to surrounding objects, creating a detailed 2D map of the environment. The video shows lines of different colors signifying various objects detected by the LiDAR. The darker the color, the more likely it is recognized as a solid object. This color coding helps in distinguishing between different types of objects and understanding their solidity. The LiDAR data is visualized in RViz, a 2D visualization tool for ROS, which allows us to see the environment from the robot's perspective and ensures accurate mapping and obstacle detection.<br>
                        </p>
                        <iframe src="https://youtube.com/embed/Bpg9jhSw23E" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_1', this)" class="expandable plus"> Update Demo 1</button>
            <div class="hidden btn_group" id="Demo_1">
                <p>
                    In this demo, we focused on improving the accuracy of the robot's movements and integrating more sensors.
                </p>
                <img src="demo_1.png" alt="Map of a room created during Demo 1" class="small_img">
                <p>
                    For the first demo, we attempted to solidify the LiDAR integration with the Mini Pupper to generate a map outside of a simulation. We explored two approaches for integrating the LiDAR: initially reviewing and utilizing the existing Mini Pupper SLAM documentation to begin generating maps, and subsequently attempting to modify or develop our own SLAM implementation to achieve better control over the mapping process. The resulting map demonstrates the Mini Pupper's ability to accurately navigate and map its environment. By integrating additional sensors and refining the robot's movements, we were able to create a detailed and accurate representation of the room.<br>
                </p>
                <br>
            </div>
        </div>

        <div class="section">
            <button onclick="toggle_visibility('Demo_2', this)" class="expandable plus"> Update Demo 2</button>
            <div class="hidden btn_group" id="Demo_2">
                <p>
                    Our final demo showcases the complete implementation of the SLAM system on the Mini Pupper, including navigation.<br>
                </p>
                
                <div class="section">
                    <button onclick="toggle_visibility('final_presentation', this)" class="expandable plus"> Presentation</button>
                    <div class="hidden btn_group" id="final_presentation">
                        <p>
                            The presentation shown below is the last update on the project done before the class. As such there is reflective content with a bent toward finality. Any further work
                            was optional.
                        </p>
                        <object data="Final Project Update.pdf" type="application/pdf" width="100%" height="500"></object>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('videos_1', this)" class="expandable plus"> Update Videos</button>
                    <div class="hidden btn_group" id="videos_1">
                    
                        <iframe src="https://youtube.com/embed/kvn6AokmaWs" allowfullscreen></iframe>

                        <p>
                            There are a few things to note about the video above.
                        </p>
                        <ul>
                            <li>
                                The "leash" as some people have called it was due to the pupper's battery dieing right as we started recording for the demo.
                                The pupper has a power cord and so we got around the issue by connecting an extension cord. John was holding the power cord to keep it from interfearing with the pupper's motion.
                            </li>
                            <li>
                                The pupper is much closer to the ground than in previous videos. After discussing the pupper's proclivity to tipping over during turns with the developer we learned that
                                the center of mass can just be lowered. This also keeps the pupper from kicking the ground as much.
                            </li>
                            <li>
                                The teleop_twist_keyboard command is different than what is given out of the box. I made some external libraries to "think" through the various functions that we still needed for
                                localization and mapping. One of those changes draws graphs (in png form) that allows me to see what the pupper was thinking about its environment. This drawing step causes the pupper
                                to appear to be thinking for longer. Though part of the extra time is due to the pupper just not being a fast thinker.
                            </li>
                            <li>
                                An occupancy grid was created by ROS using lidar before the start of the video. ROS's map.yaml which should store the pupper's location on the map is not correct. This causes there
                                to be a "where am i" in the image. Though we did solve the question, the accuracy is not perfect and therefore the pupper in the video actually thinks there is a table infront of it,
                                which is why it does so much turning.
                            </li>
                        </ul>

                        <p>
                            Regardless of the setbacks, the 4 days that led up to this video were insane. A ton of effort and brainstorming was put in to find clever out of the box ways of circumventing our lack of
                            ros2 knowledge, challenges the pupper put up, and how to make something presentable so we could finish the assignment. Luckily after spending most of the term working on ros2 a method clicked
                            and all that was left was to just do it. We found the problems and tackled them one by one.
                        </p>
                    </div>
                </div>

                <div class="section">
                    <button onclick="toggle_visibility('solution', this)" class="expandable plus"> Brainstorming a Solution</button>
                    <div class="hidden btn_group" id="solution">
                        <p>
                            After many hours of work in the ros environment, optimizing some of the ros launch files, editing the rviz settings to more closely match our aesthetic needs
                            I realized that I was not going to be able to finish the project entirely in ROS. I just have too little knowledge about making my own ros packages to utilize
                            ros to any inteligable extent. A thought popped into my head, what if we did not have to finish the project in ros.<br>

                            <br>

                            Callen Sun (a developer at MangDang) had published a new readme as well as some cleanup for some of the launch files the morning of this thought process. In that
                            updated readme was a method for converting an occupancy to a .pgm (a file). Pgm files can be read trivially by imageio. John worked with them in ROB 456 for
                            simulated pathfinding. The following methods would be needed to bridge the gap between what ros2 and our full project description.
                        </p>
                        <ul>
                            <li>take_snapshot, create an occupancy grid image of the environment using ros.</li>
                            <li>where_am_i, where is the pupper in the room base on the newest snapshot.</li>
                            <li>where_is_good, where should the pupper go next to ensure full exploration.</li>
                            <li>get_path, find a pixel path from the robots location to a goal location.</li>
                            <li>move, turn a path into a series of commands. As movement is toggled rather than held, commands should be direction and then duration.</li>
                            <li>get_completion_rate, based on an overall map, what percent has been explored.</li>
                        </ul>
                        <p>
                            There were some caviots that needed to be addressed early. If any of the following were not to work the idea would need to be rethought.
                        </p>
                        <ol>
                            <li>
                                A python program must be able to reasonably launch the ros slam program (which runs indefinitely) and then stop it after it has created a satisfactory
                                occupancy grid.
                            </li>
                            <li>The program must be able to tell where the pupper is from the lidar.</li>
                            <li>
                                Movement of the pupper must be consistant enough that drift won't cause major descrepencies between the map and the pupper. We're not using
                                dead reckoning, but we don't want localization to be impossible.
                            </li>
                        </ol>
                        <p>
                            John tackled the first problem immidiately. It turns out that subprocessing.Popen opens its own terminals. Since each terminal is unique, commands like jobs and kill %1
                            will not work. Luckily launching popen is a threaded process and therfore I can use things like wait and time.sleep to ensure the criteria I am looking for in the external
                            program are met.<br>

                            <br>

                            As exhilarating as problem one was to solve, problem two was in a class of its own. Problem two was as beautiful in its simplicity as it was in its aesthetic.
                        </p>
                        <img src="Lidar_Image.jpg" class="medium_img">
                        <p>
                            There is a map.yaml file, but without an imu, the cordinates for the pupper are incorrect (even when standing still). Fortunately a common artifact of lidars are the
                            diagonals that appear to point toward the pupper (as that's where the lasers originated).
                        </p>
                        <img src="Lidar_Grouped_Dots.jpg" class="medium_img">
                        <p>
                            If the region between free space and unknown is converted into groups the groups could be colored unique colors to make them human readible.
                        </p>
                        <img src="Lidar_Grouped_Dot_Lines.jpg" class="medium_img">
                        <p>
                            Larger groups could then be converted into lines.
                        </p>
                        <img src="Lidar_Grouped_Dot_Line_Intersections.jpg" class="medium_img">
                        <p>
                            Given the beginning and the end, m and b can be calculated for y=mx+b. If the map is stepped through at small enough x steps every intersection can be found. The average
                            of the intersections should be the robot position (marked by the red cross shown in all 4 images). Unfortunately this is not perfect. Boring torain can cause fewer lines.
                            Worse, a small area may result in no lines, which would result in no pupper location.<br>
                        </p>

                        <br>

                        <p>
                            Finally, the third problem was actually not an issue. A couple weeks ago Afreez (developer at Mangdang) had suggested lowering the pupper to lower the center of mass.
                            This resulted in less ground kicking and a lower chance of tipping over.<br>
                        </p>

                        <br>

                        <p>
                            The observant reader will notice that by solving these problems take_snapshot and part of where_am_i were solved above. Finding where the pupper is in a lidar reading is only
                            part of the answer. The next step would be to use the landmarks and expected location to match up with the rest of the room's map. Fortunately, for demo 2 (pathfind to a
                            selected location) we only needed a single lidar scan. where_is_good and get_path were able to be easily modified for our lidar maps. Now that all of these pieces were together,
                            we combined them in the teleop_twist_keyboard which controls the movement of the pupper via keyboard. John added a space button and a g button so that the keyboard kept
                            part of its original functionality. Tapping space results in a snapshot and g says to go toward a targeted location. Some measured runs were used to calculate unit conversion.
                        </p>
                        <ul>
                            <li>3 feet/20 pixels</li>
                            <li>3 feet/10.96 seconds</li>
                            <li>720 degrees/29.33 seconds</li>
                            <li>360 degrees/14.88 seconds</li>
                        </ul>
                        <p>The culmination of this work resulted in the video below.</p>
                        <iframe src="https://youtube.com/embed/kvn6AokmaWs" allowfullscreen></iframe>
                        <p>Note this is the same video shown in the previous tab.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
